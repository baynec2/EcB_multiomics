---
title: "machine_learning_analysis"
format: 
  html:
    theme: flatly
    code-fold: true
    code-tools: true
editor: visual
---

# Machine Learning Analysis

Here we will be performing a machine learning analysis of our proteomics and metabolomics data.

We intend to evaluate several models focusing on 3 comparisons:

1.  Healthy vs Infected patients
2.  E.faecalis vs E.faecium infected patients
3.  Mortality vs Survival.

Our dataset has low n but high p, so we will apply a lasso regression in hopes that this will prevent overfitting of the data.

## Loading/Formatting Data

```{r}
#| echo: false
#| eval: false
# Cleaning the data and saving to file for easy reference. 

proteomics = read_csv("../../01_r_processed_data/quant_msstats_protein_level_data.csv") %>% 
  dplyr::select(-(1:4)) %>% 
  dplyr::select(-sample_id,-Condition) %>% 
  tidyr::pivot_wider(names_from = BioReplicate,values_from = Abundance) %>% 
  dplyr::filter(orientation == "sp") %>% 
  na.omit()

write_csv(proteomics,"proteomics_data_cleaned.csv")

metabolo_id = read_csv("../../00_r_input_data/gnps_annotations.csv") %>% 
  select(row_id,compound_name = compound_name_cleaned)

metabolomics = read_csv("../../01_r_processed_data/normalized_metabolomics.csv") %>% 
  select(row_id,sample_id,norm_value) %>% 
  left_join(metabolo_id,by = "row_id") %>% 
  pivot_wider(names_from = sample_id,values_from = norm_value)


write_csv(metabolomics, "metabolomics_data_cleaned.csv")

```

```{r}
#| label: load-libraries

library(dplyr)
library(tidymodels)
library(caret)
library(broom)
library(readr)
library(ggplot2)
library(thematic)

ggplot2::theme_set(theme_bw())

```

```{r}
#| label: load-data

clinical_metadata <- read_csv("clinical_metadata.csv") 
  
  # Clinical metadata with only the relevant outcomes
clinical_metadata_o = clinical_metadata %>% 
    select(sample_id, condition, infection_status,
           death_during_admission)

proteomics_data <- read_csv('proteomics_data_cleaned.csv')
metabolomics_data <- read_csv('metabolomics_data_cleaned.csv')
```

```{r}
#| label: transpose-data

# Transposing proteomics
proteomics_t <- proteomics_data %>%
  select(-orientation, -uniprotid, -locus) %>% 
  pivot_longer(2:length(.),names_to = "sample_id") %>% 
  pivot_wider(names_from = Gene, values_from = value)

# Transposing metabolomics  
metabolomics_t <- metabolomics_data %>% 
  dplyr::filter(!is.na(compound_name)) %>% 
  dplyr::mutate(row_id = paste0(row_id,":",compound_name)) %>% 
  select(-compound_name) %>% 
  pivot_longer(2:length(.),names_to = "sample_id") %>% 
  pivot_wider(names_from = row_id, values_from = value)

# Combining proteomics and metabolomics
all_t = dplyr::inner_join(proteomics_t,metabolomics_t,by = "sample_id")
```

## Model Function

Here we will define a function to do all the modeling steps. This way for each comparison, we only need to provide the model data and model recipie without repeating everything over and over.

```{r}
#| label: defining functions

# Defining the model Max built as a function, so we can easily apply it to all 
# of our comparisons.
our_model = function(model_data,
                     outcome,
                     model_recipe){
# Setting seed at 2 to make reproducible.  
set.seed(2)
# Create train/test split
train_test_split <- initial_split(model_data,prop = 0.8,
                                  strata = outcome)
train_data <- training(train_test_split)
test_data <- testing(train_test_split)

# Setting a logistic regression. 
log_reg <- logistic_reg(
  mixture = 1, # Lasso regression
  penalty = tune() # testing a few different penalty values
) %>% 
  set_engine("glmnet") 

# Creating a model workflow
model_workflow <- workflow() %>% 
  add_recipe(model_recipe) %>% 
  add_model(log_reg)

# 5 fold cross validation
cv_folds <- vfold_cv(train_data, v = 5)

# Tune the penalty parameter
tune_res <- tune_grid(
  model_workflow,
  resamples = cv_folds,
  grid = 5,     # Try 5 values of the penalty
  metrics = metric_set(roc_auc, accuracy), # generate performance metrics for roc_auc & accuracy
  control = control_grid(save_pred = TRUE)
)

# Show performance metrics and get the best model
best_result <- tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  slice_max(mean, n = 1) %>%
  head(1)

# Finalize the workflow using the model workflow and our best performing model
final_workflow <- finalize_workflow(model_workflow, best_result)

# Fit the Final Model on the Training Data
fit_workflow <- fit(final_workflow, train_data)

# Coefficients for features that seem to matter to the model, the rest have been set to 0 via lasso regression
model_summary <- tidy(fit_workflow)
model_summary_out = model_summary %>%
  filter(estimate != 0)

 # Make predictions on the test data
  predictions <- predict(fit_workflow, test_data, type = "prob") %>%
    bind_cols(predict(fit_workflow, test_data)) %>%
    bind_cols(test_data)
  
  # Generate confusion matrix
  pred <- predictions %>% select(outcome) %>% pull() %>% as.factor()
  res <- predictions %>% select(.pred_class) %>% pull() %>% as.factor()
  confusion_matrix <- confusionMatrix(pred, res)
  confusion_matrix_out <- confusion_matrix$table
  
  # Plotting coefficients (Lasso-selected features)
  p1 <- model_summary_out %>% 
    ggplot(aes(reorder(term, estimate), estimate)) +
    geom_point() +
    coord_flip()+
    xlab("term")
  
  # Return relevant outputs
  out <- list(
    model_summary = model_summary_out,
    test_results = predictions,
    model_summary_plot = p1,
    confusion_matrix = confusion_matrix_out
  )
  return(out)
}

# Extracting the ROC curves from the output of our_model()
extract_roc = function(list,outcome,prediction_col){
  
  # Convert the outcome variable to a factor
  predictions <- list$test_results %>% 
    mutate(!!outcome := as.factor(!!sym(outcome)))
  
  # Generate ROC curve data
  roc_data <- roc_curve(predictions, !!sym(outcome), !!sym(prediction_col))
  roc_auc <- roc_auc(predictions, !!sym(outcome), !!sym(prediction_col))
  
  # Plot ROC curve
  p1 <- autoplot(roc_data) +
    ggtitle(paste0("AUC = ", round(roc_auc$.estimate, 2)))
  
  return(p1)
}
```

## Healthy vs Infected

First, let's see how well our model is able to predict healthy vs infected.

```{r}
#| label: generate-model-data
#| echo: false
#| include: true
model_data <- clinical_metadata_o %>%
  merge(all_t, by = "sample_id") 

```

```{r}
#| label: create-healthy-v-infected-model-recipe
#| echo: false
#| include: false

# Recipe does all of the feature engineering so that we don't have to do anything else to the input data
h_v_i_model_recipe <- recipe(infection_status ~ ., data = model_data) %>%
  update_role(sample_id, new_role = "id") %>% 
  update_role(condition, new_role = "id") %>% 
  update_role(death_during_admission, new_role = "id") %>%  # id role tells the model not to actually incorporate it as a feature
  update_role(infection_status, new_role = "outcome") %>% # specify target 
  step_dummy(all_nominal_predictors()) %>% # convert string to dummy variables
  step_normalize(all_numeric_predictors()) # normalize numeric features
```

```{r}
#| label: view ROC 

h_v_i_res <- our_model(model_data,"infection_status",h_v_i_model_recipe)

extract_roc(h_v_i_res,"infection_status",".pred_infected")
```

**Figure 1.** Here we see that the model performs perfectly.

What features are important for this model?

```{r}
#| label: view estimates 

h_v_i_res$model_summary_plot
```

**Figure 2.** Here we see the features that are important for the model. Several of these features are likely artifacts of the way the samples were collected. Phtalic anhydride and phthalate are plasticizers. These are almost certainly due to the fact that these samples were collected in different types fo plastic tubes. This was a point the reviewers brought up.

We should either run the model with these features removed, or entirely remove the metabolomics data from this comparison.

Also, note the value of the intercept. This is a reasonable value as there is some class imbalance here since we have 76 infected patients and only 29 uninfected.

```{r}
# Recipe does all of the feature engineering so that we don't have to do anything else to the input data
h_v_i_model_recipe2 <- recipe(infection_status ~ ., data = model_data) %>%
  update_role(sample_id, new_role = "id") %>% 
  update_role(condition, new_role = "id") %>% 
  update_role(`RID_29112:bis(2-ethylhexyl) phthalate` , new_role = "id") %>% 
  update_role(`RID_21438:decynediol, tetramethyl, di(2-hydroxyethyl) ether` , new_role = "id") %>%            update_role(`RID_10819:pentapropylene glycol` , new_role = "id") %>% 
  update_role(`RID_22644:phthalic anhydride` , new_role = "id") %>% 
  update_role(death_during_admission, new_role = "id") %>%  # id role tells the model not to actually incorporate it as a feature
  update_role(infection_status, new_role = "outcome") %>% # specify target 
  step_dummy(all_nominal_predictors()) %>% # convert string to dummy variables
  step_normalize(all_numeric_predictors()) # normalize numeric features
```

```{r,fig.height =3, fig.width= 3}
h_v_i_res2 <- our_model(model_data,"infection_status",h_v_i_model_recipe2)

p1= extract_roc(h_v_i_res2,"infection_status",".pred_infected")

p1

ggsave("healthy_vs_infected_ROC.pdf",p1,width = 3,height = 3)

ggsave("../../03_supplemental/s11_machine_learning_proteomics_metabolomics/S11_a_1.pdf",p1,width = 3,height = 3)
```

**Figure 3.** Model still performs perfectly with problematic metabolites removed.

```{r,fig.height = 3.5, fig.width = 7}
p2 = h_v_i_res2$model_summary_plot

p2

ggsave("healthy_vs_infected_estimates.pdf",p2, height = 3.5, width = 7 )
ggsave("../../03_supplemental/s11_machine_learning_proteomics_metabolomics/S11_a_2.pdf",p2,width = 7,height = 3.5)
```

**Figure 4.** Here we can see the features important to this new model.

## *E. faecalis* vs *E. faecium*

Now let's see how well the modeling performs for distinguishing *E.faecalis* from *E.faecium*

```{r}
#| label: faecalis v faecium model 

# Have to discard the healthy data here 
model_data <- clinical_metadata_o %>%
  merge(all_t, by = "sample_id") %>% 
  dplyr::filter(condition != "healthy")

outcome = "infection_status"

# Recipe does all of the feature engineering so that we don't have to do anything else to the input data
faecalis_v_faecium_recipe <- recipe(condition ~ ., data = model_data) %>%
  update_role(sample_id, new_role = "id") %>% 
  update_role(infection_status, new_role = "id") %>% 
  update_role(death_during_admission, new_role = "id") %>%  # id role tells the model not to actually incorporate it as a feature
  update_role(condition, new_role = "outcome") %>% # specify target 
  step_dummy(all_nominal_predictors()) %>% # convert string to dummy variables
  step_normalize(all_numeric_predictors()) # normalize numeric features

faecalis_v_faecium_res <- our_model(model_data,"condition",faecalis_v_faecium_recipe)
```

```{r,fig.height = 3, fig.width = 3}
#| label: model ROCs
p3 = extract_roc(faecalis_v_faecium_res,"condition",".pred_faecalis")

p3 

ggsave("faecalis_v_faecium_ROC.pdf",p3, height= 3, width = 3)

ggsave("../../03_supplemental/s11_machine_learning_proteomics_metabolomics/S11_b_1.pdf",p3,width = 3,height = 3)
```

**Figure 5.** Here we can see the ROC curve for the E.facalis vs E.faecium prediction. The model looks like it is performing okay here. This isn't good enough to be clinically useful really.

Note that a simple model where you assume everyone has E. faecalis gets it right 57% of the time since there are 43 patients with faecalis and only 32 with faecium.

```{r,fig.height = 3.5, fig.width = 7 }
#| label: model estimates
#| 
p4 = faecalis_v_faecium_res$model_summary_plot
p4

ggsave("faecalis_vs_faecium_estimates.pdf",p4, height = 3.5, width = 7 )
ggsave("../../03_supplemental/s11_machine_learning_proteomics_metabolomics/S11_b_2.pdf",p4,width = 7,height = 3.5)
```

**Figure 6.** Here we can see the features that are most important to the model. Looks very similar to the previous analysis. Several of these proteins and metabolites are immune related.

Signal we are picking up here is probably that people with faecium infections are more likely to be immmunocompromised (this is already established in the literature.)

### Metadata alone model

I am curious how well a model made with just the metadata would perform.

I would expect the MIC testing to do a good job of predicting faecium, as facieum isolates are more likely to be ampicillin and vancomycin resistant.

```{r,fig.height = 3, fig.width = 3}
model_data <- clinical_metadata %>% 
  filter(condition != "healthy") %>% 
  # antimicrobial testing was not perfomred for this patient
  filter(sample_id != "S18") %>% 
  #getting rid of uninformative columns. Either very little variance, or too many levels to be useful 
  select(-pathogen,-patient_id,-pathogen_source,-infection_status,
         -bacteremia_recurrence,-race,-death_at_one_year,
         -peripheral_vascular_disease,-dementia,-hemiplegia,
         -highly_active_antiretroviral_therapy,
         -source) %>% 
  mutate(transplant_type = case_when(transplant_type == "none" ~ "none",
                                     TRUE ~ "transplant")) %>% 
  select_if(~ !any(is.na(.))) %>% 
  select(-antibiotic_therapy_before_48_hrs,-antibiotic_therapy_after_48_hrs) %>% 
   mutate(across(c(history_of_myocardial_infarction, 
                   congestive_heart_failure, 
                   history_of_stroke_or_transient_ischemic_attack, 
                   smoking, copd, peptic_ulcer_disaese, 
                   mod_to_severe_chromic_kidney_disease, 
                   hodgkin_disease, leukemia_or_lymphoma, 
                   icu_admission, mechanical_ventilation, 
                    polymicrobial_bacteremia, 
                   thrombocytopenia_plt_50, hypotension,sensitive_to_vancomycin), 
                as.factor)) %>% 
  mutate(wbc = as.numeric(wbc))


faecalis_v_faecium_m_model_recipe <- recipe(condition ~ ., data = model_data) %>%
  update_role(sample_id, new_role = "id") %>% 
  update_role(death_during_admission, new_role = "id") %>%  # id role tells the model not to actually incorporate it as a feature
  update_role(condition, new_role = "outcome") %>% # specify target 
  step_novel() %>% 
  step_dummy(all_nominal_predictors()) %>% # convert string to dummy variables
  step_impute_knn(wbc) %>% 
  step_normalize(all_numeric_predictors()) # normalize numeric features

condition_metadata_res = our_model(model_data,"condition",
                                   faecalis_v_faecium_m_model_recipe)

p5 = extract_roc(condition_metadata_res,"condition",".pred_faecalis")

p5

ggsave("faecalis_vs_faecium_md_roc_1.pdf",p5, height = 3, width = 3)
ggsave("../../03_supplemental/s12_machine_learning_clinical_metadata/S12_a_1.pdf",p5, width = 3,height = 3)
```

**Figure 7.** The model based on the metadata alone performs well. It actually performs better than our metabolomics/proteomics model.

What features are important?

```{r,fig.height = 3.5, fig.width = 5.5}
p6 = condition_metadata_res$model_summary_plot

p6

ggsave("faecalis_vs_faecium_md_estimate_1.pdf",p6, height = 3.5, width = 5.5)
ggsave("../../03_supplemental/s12_machine_learning_clinical_metadata/S12_a_2.pdf",p6,
       width = 5.5,
       height = 3.5)
```

**Figure 8.** Here we can see that the model values amp_mic as the most important features.

For all intents and purposes, this is cheating because by the time you perform a MIC, you already know what microbe is underlying the infection.

The whole point of our study was to use methods that would allow you to perform a prediction faster (1-3 days in advance

What if we remove variables that allow you to predict microbe identity (and were captured several days after admittance) from the metadata model?

```{r}
faecalis_v_faecium_m_model_recipe2 <- recipe(condition ~ ., data = model_data) %>%
  update_role(sample_id, new_role = "id") %>% 
  update_role(death_during_admission, new_role = "id") %>%  # id role tells the model not to actually incorporate it as a feature
  update_role(condition, new_role = "outcome") %>%# specify target 
  update_role(pathogen_type,new_role = "id") %>% 
  update_role(sensitive_to_vancomycin,new_role = "id") %>% 
  update_role(amp_mic,new_role = "id") %>% 
  update_role(vanc_mic,new_role = "id") %>% 
  step_novel() %>% 
  step_dummy(all_nominal_predictors()) %>% # convert string to dummy variables
  step_impute_knn(wbc) %>% 
  step_normalize(all_numeric_predictors()) # normalize numeric features

condition_metadata_res2 = our_model(model_data,"condition",
                                   faecalis_v_faecium_m_model_recipe2)


p7 = extract_roc(condition_metadata_res2,"condition",".pred_faecalis")

p7
ggsave("../../03_supplemental/s12_machine_learning_clinical_metadata/S12_b_1.pdf",p7, width = 3 ,height = 3)

ggsave("faecalis_vs_faecium_md_roc_2.pdf",p7, height = 3, width = 3)


p8 = condition_metadata_res2$model_summary_plot
ggsave("../../03_supplemental/s12_machine_learning_clinical_metadata/S12_b_2.pdf",p8, width = 5 ,height = 3.5)


```

**Figure 8.** When we do this, we find that that model performs similarily to our proteomics/metabolomics model

## Mortality vs Survival

Now lets see how well we can predict mortality and survival using the proteomics and metabolomics data.

```{r}
# Have to discard the healthy data here 
model_data <- clinical_metadata_o %>%
  merge(all_t, by = "sample_id") %>% 
  dplyr::filter(condition != "healthy") %>% 
  dplyr::mutate(death_during_admission = as.factor(death_during_admission))


# Recipe does all of the feature engineering so that we don't have to do anything else to the input data
mortality_vs_survival_recipe <- recipe(death_during_admission ~ ., data = model_data) %>%
  update_role(sample_id, new_role = "id") %>% 
  update_role(infection_status, new_role = "id") %>% 
  update_role(death_during_admission, new_role = "outcome") %>%  # id role tells the model not to actually incorporate it as a feature
  update_role(condition, new_role = "id") %>% # specify target 
  step_dummy(all_nominal_predictors()) %>% # convert string to dummy variables
  step_normalize(all_numeric_predictors()) # normalize numeric features


mortality_vs_survival_res = our_model(model_data,"death_during_admission",mortality_vs_survival_recipe)
```

```{r,fig.height = 3, fig.width = 3}
p11 = extract_roc(mortality_vs_survival_res,"death_during_admission",".pred_FALSE")

p11 

ggsave("mortality_vs_survival_ROC.pdf",p11, height = 3, width = 3)
ggsave("../../03_supplemental/s11_machine_learning_proteomics_metabolomics/S11_c_1.pdf",p11, width = 3 ,height = 3)

```

**Figure 12.** Here is a ROC curve of our model. It looks like it is reasonably accurate, but note that there is a big class imbalance here. A simple model where no patients die would get it right 76% of the time (57/75). Our fancy model is only slightly slightly improved on this. We are really hindered by our ability to only test a total of 3 true positives (due to the small sample size).

```{r}
p12 = mortality_vs_survival_res$confusion_matrix

p12

```

**Table 1.** Here a confusion matrix highlights this class imbalance.

```{r,fig.height = 3.5, fig.width = 8 }
p13 = mortality_vs_survival_res$model_summary_plot

p13

ggsave("mortality_vs_survival_estimate.pdf",p13, height = 3.5, width = 6)
ggsave("../../03_supplemental/s11_machine_learning_proteomics_metabolomics/S11_c_2.pdf",p13, width = 8 ,height = 3.5)
```

**Figure 13.** Here we can see the features important for the model. Note the size of the intercept.

### Metadata alone model

I am curious how well a model made with just the metadata would perform.

```{r,fig.height = 3, fig.width = 3}
model_data <- clinical_metadata %>% 
  filter(condition != "healthy") %>% 
  # antimicrobial testing was not perfomred for this patient
  filter(sample_id != "S18") %>% 
  #getting rid of uninformative columns. Either very little variance, or too many levels to be useful 
  select(-pathogen,-patient_id,-pathogen_source,-infection_status,
         -bacteremia_recurrence,-race,-death_at_one_year,
         -peripheral_vascular_disease,-dementia,-hemiplegia,
         -highly_active_antiretroviral_therapy,
         -source) %>% 
  mutate(transplant_type = case_when(transplant_type == "none" ~ "none",
                                     TRUE ~ "transplant")) %>% 
  select_if(~ !any(is.na(.))) %>% 
  select(-antibiotic_therapy_before_48_hrs,-antibiotic_therapy_after_48_hrs) %>% 
   mutate(across(c(history_of_myocardial_infarction, 
                   congestive_heart_failure, 
                   history_of_stroke_or_transient_ischemic_attack, 
                   smoking, copd, peptic_ulcer_disaese, 
                   mod_to_severe_chromic_kidney_disease, 
                   hodgkin_disease, leukemia_or_lymphoma, 
                   icu_admission, mechanical_ventilation, 
                    polymicrobial_bacteremia, 
                   thrombocytopenia_plt_50, hypotension,sensitive_to_vancomycin,
                   death_during_admission), 
                as.factor)) %>% 
  mutate(wbc = as.numeric(wbc))


mort_m_model_recipe <- recipe(death_during_admission ~ ., data = model_data) %>%
  update_role(sample_id, new_role = "id") %>% 
  update_role(death_during_admission, new_role = "outcome") %>%  # id role tells the model not to actually incorporate it as a feature
  #update_role(condition, new_role = "outcome") %>% # specify target 
  step_dummy(all_nominal_predictors()) %>% # convert string to dummy variables
  step_impute_knn(wbc) %>% 
  step_normalize(all_numeric_predictors()) # normalize numeric features

mort_metadata_res = our_model(model_data,"death_during_admission",
                              mort_m_model_recipe)

p14 = extract_roc(mort_metadata_res,"death_during_admission",".pred_FALSE")

p14

ggsave("mortality_vs_survival_md_roc_1.pdf",p14,height = 3, width = 3)
ggsave("../../03_supplemental/s12_machine_learning_clinical_metadata/S12c_1.pdf",p14,height = 3, width = 3)
```

**Figure 14.** Here we see that the clinical metadata does better job of predicting mortality than oru

What is going on here?

```{r,fig.height = 3.5, fig.width= 5.5}
p15 = mort_metadata_res$model_summary_plot

p15

ggsave("mortality_vs_survival_md_estimate_1.pdf",p15, height = 3.5, width = 5.5)
ggsave("../../03_supplemental/s12_machine_learning_clinical_metadata/S12c_2.pdf",p15,height = 3, width = 5.5)
```

\*\*Figure 15.\* Turns out if you get admitted to ICU, that predicts mortality pretty well (obviously). Once again, this is kind of cheating, ICU admission is a feature that is only apparent after a patient is at the hospital for some time.

What if we remove this variable from the model?

```{r,fig.height = 3, fig.width = 3 }
mort_m_model_recipe <- recipe(death_during_admission ~ ., data = model_data) %>%
  update_role(sample_id, new_role = "id") %>% 
  update_role(death_during_admission, new_role = "outcome") %>%  # id role tells the model not to actually incorporate it as a feature
  update_role(icu_admission, new_role = "id") %>% # specify target 
  step_dummy(all_nominal_predictors()) %>% # convert string to dummy variables
  step_impute_knn(wbc) %>% 
  step_normalize(all_numeric_predictors()) # normalize numeric features

mort_metadata_res = our_model(model_data,"death_during_admission",
                              mort_m_model_recipe)

p16 = extract_roc(mort_metadata_res,"death_during_admission",".pred_FALSE")

p16

ggsave("mortality_vs_survival_md_roc_2.pdf",p16, height = 3, width = 3)
ggsave("../../03_supplemental/s12_machine_learning_clinical_metadata/S12d_1.pdf",p16,height = 3, width = 3)

```

**Figure 16.** If you take out the ICU admission, you lose all model performance. You actually would be better off just predicting no one dies.

```{r}
p15 = mort_metadata_res$model_summary_plot

p15

ggsave("mortality_vs_survival_md_estimate_2.pdf",p15, height = 3.5, width = 5.5)
ggsave("../../03_supplemental/s12_machine_learning_clinical_metadata/S12d_2.pdf",p15,height = 3, width = 5.5)
```

# Conclusion

1.  The results of the lasso regression models show very similar results/ interpretations to the other form of analysis that we conducted.

2.  These analyses should adress Review 2 comments appropriately,
